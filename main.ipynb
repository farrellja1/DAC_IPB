{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe3c7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf766852",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Train/'\n",
    "test_dir = 'Test/'\n",
    "\n",
    "def import_data(directory_path):\n",
    "    try:\n",
    "        all_filenames = os.listdir(directory_path)\n",
    "        csv_files = [\n",
    "            os.path.join(directory_path, filename) \n",
    "            for filename in all_filenames \n",
    "            if filename.endswith('.csv')\n",
    "        ]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{directory_path}' was not found.\")\n",
    "        return None\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"Warning: No CSV files found in the directory: {directory_path}\")\n",
    "        return None\n",
    "\n",
    "    list_of_dfs = []\n",
    "    print(f\"Found {len(csv_files)} CSV files to process...\")\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(file_path)\n",
    "            filename = os.path.basename(file_path)\n",
    "            app_name = os.path.splitext(filename)[0]\n",
    "            temp_df['App'] = app_name\n",
    "            list_of_dfs.append(temp_df)\n",
    "            print(f\"  - Processed {filename} and added '{app_name}' as App.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    print(\"\\nCombining all DataFrames...\")\n",
    "    combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    print(\"Done.\")\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d70860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 CSV files to process...\n",
      "  - Processed claude.csv and added 'claude' as App.\n",
      "  - Processed deepseek.csv and added 'deepseek' as App.\n",
      "  - Processed gemini.csv and added 'gemini' as App.\n",
      "  - Processed gpt.csv and added 'gpt' as App.\n",
      "  - Processed grok.csv and added 'grok' as App.\n",
      "  - Processed perplexity.csv and added 'perplexity' as App.\n",
      "\n",
      "Combining all DataFrames...\n",
      "Done.\n",
      "Found 6 CSV files to process...\n",
      "  - Processed claude.csv and added 'claude' as App.\n",
      "  - Processed deepseek.csv and added 'deepseek' as App.\n",
      "  - Processed gemini.csv and added 'gemini' as App.\n",
      "  - Processed gpt.csv and added 'gpt' as App.\n",
      "  - Processed grok.csv and added 'grok' as App.\n",
      "  - Processed perplexity.csv and added 'perplexity' as App.\n",
      "\n",
      "Combining all DataFrames...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_set = import_data(train_dir)\n",
    "test_set = import_data(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39074dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, text_column):\n",
    "    print(\"\\n--- Starting Text Preprocessing ---\")\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy[text_column] = df_copy[text_column].astype(str).fillna('')\n",
    "\n",
    "    print(\"Step 1: Cleaning and lowercasing text...\")\n",
    "    df_copy['cleaned_text'] = df_copy[text_column].apply(\n",
    "        lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower())\n",
    "    )\n",
    "\n",
    "    # print(\"Step 2: Tokenizing text with BertTokenizer...\")\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # df_copy['tokenized_text'] = df_copy['cleaned_text'].apply(tokenizer.tokenize)\n",
    "\n",
    "    # print(\"Step 3: Removing stop words...\")\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # df_copy['tokenized_text'] = df_copy['tokenized_text'].apply(\n",
    "    #     lambda tokens: [word for word in tokens if word not in stop_words]\n",
    "    # )\n",
    "\n",
    "    # print(\"Step 4: Applying stemming...\")\n",
    "    # stemmer = PorterStemmer()\n",
    "    # df_copy['tokenized_text'] = df_copy['tokenized_text'].apply(\n",
    "    #     lambda tokens: [stemmer.stem(word) for word in tokens]\n",
    "    # )\n",
    "\n",
    "    # if is_train:\n",
    "    #     print(\"Step 5: Identifying and removing rare words (from training data)...\")\n",
    "    #     all_words = [word for tokens in df_copy['tokenized_text'] for word in tokens]\n",
    "    #     fdist = FreqDist(all_words)\n",
    "    #     rare_words_to_remove = {word for word, count in fdist.items() if count <= 2}\n",
    "    #     print(f\"  - Found {len(rare_words_to_remove)} rare words to remove.\")\n",
    "        \n",
    "    #     df_copy['processed_text'] = df_copy['tokenized_text'].apply(\n",
    "    #         lambda tokens: [word for word in tokens if word not in rare_words_to_remove]\n",
    "    #     )\n",
    "    #     print(\"Preprocessing complete.\")\n",
    "    #     return df_copy, rare_words_to_remove\n",
    "    # else:\n",
    "    #     print(\"Step 5: Removing rare words (using provided list)...\")\n",
    "    #     if rare_words_to_remove is None:\n",
    "    #         raise ValueError(\"`rare_words_to_remove` must be provided when `is_train` is False.\")\n",
    "            \n",
    "    #     df_copy['processed_text'] = df_copy['tokenized_text'].apply(\n",
    "    #         lambda tokens: [word for word in tokens if word not in rare_words_to_remove]\n",
    "        \n",
    "    print(\"Preprocessing complete.\")\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "693e4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_set, test_size=0.2, random_state=42, stratify=train_set['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c48917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Text Preprocessing ---\n",
      "Step 1: Cleaning and lowercasing text...\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "ret = preprocess(train, text_column='Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b795802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c642e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size, text_col, label_col):\n",
    "    ds = ReviewDataset(\n",
    "        reviews=df[text_col].to_numpy(),\n",
    "        labels=df[label_col].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce4f18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 3\n",
    "TEXT_COLUMN = 'Comment'\n",
    "LABEL_COLUMN = 'Sentiment'\n",
    "EPOCHS_BASE = 3\n",
    "EPOCHS_SPECIALIST = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "BASE_MODEL_PATH = 'base_model/'\n",
    "SPECIALIST_MODELS_PATH = 'specialist_models/'\n",
    "\n",
    "def train_model(model, data_loader, optimizer, device, n_epochs):\n",
    "    \"\"\"\n",
    "    Generic training loop for a PyTorch model.\n",
    "    \"\"\"\n",
    "    model = model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}')\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    return model\n",
    "def train_base_model(train_df, device):\n",
    "    print(\"\\n--- Step 2: Training Base Model (Domain Adaptation) ---\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE, TEXT_COLUMN, LABEL_COLUMN)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    model = train_model(model, train_data_loader, optimizer, device, EPOCHS_BASE)\n",
    "    \n",
    "    print(f\"Base model training complete. Saving to {BASE_MODEL_PATH}\")\n",
    "    model.save_pretrained(BASE_MODEL_PATH)\n",
    "    tokenizer.save_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "# --- 6. Specialization: Fine-tune for Each App ---\n",
    "def train_specialist_models(train_df, device):\n",
    "    print(\"\\n--- Step 3: Training Specialist Models (Fine-tuning) ---\")\n",
    "    app_names = train_df['App'].unique()\n",
    "    \n",
    "    for app_name in app_names:\n",
    "        print(f\"\\nFine-tuning for app: {app_name}\")\n",
    "        \n",
    "        # Load the domain-adapted base model\n",
    "        tokenizer = BertTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = BertForSequenceClassification.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Filter data for the specific app\n",
    "        app_df = train_df[train_df['App'] == app_name]\n",
    "        \n",
    "        app_data_loader = create_data_loader(app_df, tokenizer, MAX_LEN, BATCH_SIZE, TEXT_COLUMN, LABEL_COLUMN)\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        model = train_model(model, app_data_loader, optimizer, device, EPOCHS_SPECIALIST)\n",
    "        \n",
    "        # Save the specialist model\n",
    "        specialist_path = os.path.join(SPECIALIST_MODELS_PATH, f'bert-specialist-{app_name}')\n",
    "        print(f\"Specialist model for {app_name} training complete. Saving to {specialist_path}\")\n",
    "        model.save_pretrained(specialist_path)\n",
    "        tokenizer.save_pretrained(specialist_path)\n",
    "\n",
    "# --- 7. Evaluation ---\n",
    "def evaluate_models(test_df, device):\n",
    "    print(\"\\n--- Step 4: Evaluating Specialist Models ---\")\n",
    "    y_reviews = []\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    app_names = test_df['App'].unique()\n",
    "\n",
    "    for app_name in app_names:\n",
    "        print(f\"Evaluating on test data for app: {app_name}\")\n",
    "        specialist_path = os.path.join(SPECIALIST_MODELS_PATH, f'bert-specialist-{app_name}')\n",
    "        \n",
    "        # Load the specialist model for this app\n",
    "        tokenizer = BertTokenizer.from_pretrained(specialist_path)\n",
    "        model = BertForSequenceClassification.from_pretrained(specialist_path)\n",
    "        model = model.to(device)\n",
    "        model = model.eval()\n",
    "\n",
    "        app_test_df = test_df[test_df['App'] == app_name]\n",
    "        \n",
    "        for _, row in app_test_df.iterrows():\n",
    "            # --- FIX: Ensure the review_text is a string to handle NaN values ---\n",
    "            review_text = str(row[TEXT_COLUMN])\n",
    "            true_label = row['Sentiment'] # Assuming LABEL_COLUMN is 'Sentiment'\n",
    "            \n",
    "            encoded_review = tokenizer.encode_plus(\n",
    "                review_text,\n",
    "                max_length=MAX_LEN,\n",
    "                add_special_tokens=True,\n",
    "                return_token_type_ids=False,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded_review['input_ids'].to(device)\n",
    "            attention_mask = encoded_review['attention_mask'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                _, prediction = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            y_reviews.append(review_text)\n",
    "            y_pred.append(prediction.item())\n",
    "            y_test.append(true_label)\n",
    "\n",
    "    print(\"\\n--- Final Evaluation Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Mixed', 'Positive']))\n",
    "    \n",
    "    # Calculate and print F1 macro score\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"\\nF1 Macro Score: {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "490b77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Step 4: Evaluating Specialist Models ---\n",
      "Evaluating on test data for app: gemini\n",
      "Evaluating on test data for app: gpt\n",
      "Evaluating on test data for app: grok\n",
      "Evaluating on test data for app: deepseek\n",
      "Evaluating on test data for app: perplexity\n",
      "Evaluating on test data for app: claude\n",
      "\n",
      "--- Final Evaluation Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.72      0.75      3504\n",
      "       Mixed       0.24      0.10      0.14      1118\n",
      "    Positive       0.93      0.97      0.95     21555\n",
      "\n",
      "    accuracy                           0.90     26177\n",
      "   macro avg       0.66      0.60      0.62     26177\n",
      "weighted avg       0.88      0.90      0.89     26177\n",
      "\n",
      "\n",
      "F1 Macro Score: 0.6175\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # # Create directories for models if they don't exist\n",
    "    # if not os.path.exists(BASE_MODEL_PATH):\n",
    "    #     os.makedirs(BASE_MODEL_PATH)\n",
    "    # if not os.path.exists(SPECIALIST_MODELS_PATH):\n",
    "    #     os.makedirs(SPECIALIST_MODELS_PATH)\n",
    "    \n",
    "    # # 2. Train the base model on all training data\n",
    "    # train_base_model(train, device)\n",
    "    \n",
    "    # # 3. Fine-tune specialist models for each app\n",
    "    # train_specialist_models(train, device)\n",
    "    \n",
    "    # # 4. Evaluate the specialist models on the locked-away test set\n",
    "    evaluate_models(val, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e1d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Step 5: Predicting on New Data ---\n",
      "Loading model for app: claude\n",
      "Loading model for app: deepseek\n",
      "Loading model for app: gemini\n",
      "Loading model for app: gpt\n",
      "Loading model for app: grok\n",
      "Loading model for app: perplexity\n",
      "\n",
      "--- Prediction Results ---\n",
      "       CommentId                                            Comment  \\\n",
      "0              1  So this length limit issue has ruined Claude f...   \n",
      "1              2                        Great Ai consultation tool.   \n",
      "2              3  I am a pro user and I think there is a bug in ...   \n",
      "3              4                                   I love Claude AI   \n",
      "4              5  No access to live data without subscription (p...   \n",
      "...          ...                                                ...   \n",
      "45966       5347                                            amazing   \n",
      "45967       5348  used this app for a while, enjoyed it a lot, g...   \n",
      "45968       5349  it is for knowledge and helping for speech or ...   \n",
      "45969       5350                                        fantastic 😍   \n",
      "45970       5351                                         Great tool   \n",
      "\n",
      "                        At   AppVersion         App  predicted_sentiment  \n",
      "0      2025-06-22 22:55:09  1.250609.27      claude                    0  \n",
      "1      2024-09-14 06:07:28  1.240909.27      claude                    2  \n",
      "2      2024-11-19 13:15:37  1.241118.17      claude                    0  \n",
      "3      2024-12-06 19:03:26   1.241125.6      claude                    2  \n",
      "4      2025-03-05 20:06:49  1.250303.27      claude                    0  \n",
      "...                    ...          ...         ...                  ...  \n",
      "45966  2024-11-19 18:11:23       2.18.2  perplexity                    2  \n",
      "45967  2025-05-01 18:19:08       2.44.0  perplexity                    2  \n",
      "45968  2025-06-22 11:09:12       2.48.1  perplexity                    2  \n",
      "45969  2024-02-27 12:37:01       2.11.1  perplexity                    2  \n",
      "45970  2024-02-01 18:06:54       2.10.0  perplexity                    2  \n",
      "\n",
      "[45971 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "def predict_on_new_data(predict_df, device):\n",
    "    \"\"\"\n",
    "    Makes predictions on a new, unlabeled DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 5: Predicting on New Data ---\")\n",
    "    predictions = []\n",
    "    app_names = predict_df['App'].unique()\n",
    "    \n",
    "    # Load each specialist model once to avoid reloading in the loop\n",
    "    models = {}\n",
    "    tokenizers = {}\n",
    "    for app_name in app_names:\n",
    "        print(f\"Loading model for app: {app_name}\")\n",
    "        specialist_path = os.path.join(SPECIALIST_MODELS_PATH, f'bert-specialist-{app_name}')\n",
    "        if os.path.exists(specialist_path):\n",
    "            tokenizers[app_name] = BertTokenizer.from_pretrained(specialist_path)\n",
    "            models[app_name] = BertForSequenceClassification.from_pretrained(specialist_path)\n",
    "            models[app_name] = models[app_name].to(device)\n",
    "            models[app_name] = models[app_name].eval()\n",
    "        else:\n",
    "            print(f\"Warning: No specialist model found for app '{app_name}'. Skipping its predictions.\")\n",
    "\n",
    "    # Iterate through the dataframe to make predictions\n",
    "    for _, row in predict_df.iterrows():\n",
    "        review_text = str(row[TEXT_COLUMN])\n",
    "        app_name = row['App']\n",
    "        \n",
    "        if app_name in models:\n",
    "            tokenizer = tokenizers[app_name]\n",
    "            model = models[app_name]\n",
    "            \n",
    "            encoded_review = tokenizer.encode_plus(\n",
    "                review_text,\n",
    "                max_length=MAX_LEN,\n",
    "                add_special_tokens=True,\n",
    "                return_token_type_ids=False,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded_review['input_ids'].to(device)\n",
    "            attention_mask = encoded_review['attention_mask'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                _, prediction = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.append(prediction.item())\n",
    "        else:\n",
    "            predictions.append(None) # Append None if no model was found for the app\n",
    "\n",
    "    predict_df['predicted_sentiment'] = predictions\n",
    "    return predict_df\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create directories for models if they don't exist\n",
    "    if not os.path.exists(BASE_MODEL_PATH):\n",
    "        os.makedirs(BASE_MODEL_PATH)\n",
    "    if not os.path.exists(SPECIALIST_MODELS_PATH):\n",
    "        os.makedirs(SPECIALIST_MODELS_PATH)\n",
    "\n",
    "\n",
    "    # Get predictions\n",
    "    prediction_results_df = predict_on_new_data(test_set, device)\n",
    "    \n",
    "    print(\"\\n--- Prediction Results ---\")\n",
    "    print(prediction_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5753c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_submission(results_df):\n",
    "    \"\"\"\n",
    "    Formats the prediction results into the required submission format.\n",
    "    \"\"\"\n",
    "    print(\"\\nFormatting data for submission...\")\n",
    "    \n",
    "    # Define the desired order for the apps\n",
    "    app_order = ['gpt', 'claude', 'deepseek', 'gemini', 'grok', 'perplexity']\n",
    "    \n",
    "    # Convert 'App' column to a categorical type with the specified order\n",
    "    results_df['App'] = pd.Categorical(results_df['App'], categories=app_order, ordered=True)\n",
    "    \n",
    "    # Sort the DataFrame first by the custom app order, then by CommentId\n",
    "    sorted_df = results_df.sort_values(by=['App', 'CommentId'])\n",
    "    \n",
    "    submission_df = pd.DataFrame()\n",
    "    # Create the 'CommentId' by combining 'App' and the original 'CommentId'\n",
    "    submission_df['CommentId'] = sorted_df['App'].astype(str) + '_' + sorted_df['CommentId'].astype(str)\n",
    "    # Rename the 'predicted_sentiment' column to 'Sentiment'\n",
    "    submission_df['Sentiment'] = sorted_df['predicted_sentiment']\n",
    "    \n",
    "    print(\"Formatting complete.\")\n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "783b7b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatting data for submission...\n",
      "Formatting complete.\n"
     ]
    }
   ],
   "source": [
    "submission = format_for_submission(prediction_results_df)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
