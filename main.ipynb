{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3697e3a",
   "metadata": {},
   "source": [
    "## **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e09f5",
   "metadata": {},
   "source": [
    "## **Dataset Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf766852",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Train/'\n",
    "test_dir = 'Test/'\n",
    "\n",
    "def import_data(directory_path):\n",
    "    try:\n",
    "        all_filenames = os.listdir(directory_path)\n",
    "        csv_files = [\n",
    "            os.path.join(directory_path, filename) \n",
    "            for filename in all_filenames \n",
    "            if filename.endswith('.csv')\n",
    "        ]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{directory_path}' was not found.\")\n",
    "        return None\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"Warning: No CSV files found in the directory: {directory_path}\")\n",
    "        return None\n",
    "\n",
    "    list_of_dfs = []\n",
    "    print(f\"Found {len(csv_files)} CSV files to process...\")\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(file_path)\n",
    "            filename = os.path.basename(file_path)\n",
    "            app_name = os.path.splitext(filename)[0]\n",
    "            temp_df['App'] = app_name\n",
    "            list_of_dfs.append(temp_df)\n",
    "            print(f\"  - Processed {filename} and added '{app_name}' as App.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    print(\"\\nCombining all DataFrames...\")\n",
    "    combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    print(\"Done.\")\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12d70860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 CSV files to process...\n",
      "  - Processed claude.csv and added 'claude' as App.\n",
      "  - Processed deepseek.csv and added 'deepseek' as App.\n",
      "  - Processed gemini.csv and added 'gemini' as App.\n",
      "  - Processed gpt.csv and added 'gpt' as App.\n",
      "  - Processed grok.csv and added 'grok' as App.\n",
      "  - Processed perplexity.csv and added 'perplexity' as App.\n",
      "\n",
      "Combining all DataFrames...\n",
      "Done.\n",
      "Found 6 CSV files to process...\n",
      "  - Processed claude.csv and added 'claude' as App.\n",
      "  - Processed deepseek.csv and added 'deepseek' as App.\n",
      "  - Processed gemini.csv and added 'gemini' as App.\n",
      "  - Processed gpt.csv and added 'gpt' as App.\n",
      "  - Processed grok.csv and added 'grok' as App.\n",
      "  - Processed perplexity.csv and added 'perplexity' as App.\n",
      "\n",
      "Combining all DataFrames...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_set = import_data(train_dir)\n",
    "test_set = import_data(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39074dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, text_column):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy[text_column] = df_copy[text_column].astype(str).fillna('')\n",
    "    df_copy['cleaned_text'] = df_copy[text_column].apply(\n",
    "        lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower())\n",
    "    )\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "693e4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_set, test_size=0.2, random_state=42, stratify=train_set['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26c48917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Text Preprocessing ---\n"
     ]
    }
   ],
   "source": [
    "ret = preprocess(train, text_column='Comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e59d3d",
   "metadata": {},
   "source": [
    "## **Modelling & Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b795802",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLUMN = 'Comment'\n",
    "APP_VERSION_COLUMN = 'AppVersion'\n",
    "AT_COLUMN = 'At'\n",
    "LABEL_COLUMN = 'Sentiment'\n",
    "BASE_MODEL_NAME = 'bert-base-uncased'\n",
    "BASE_MODEL_PATH = './models/base_model_with_metadata2/'\n",
    "SPECIALIST_MODELS_PATH = './models/specialist_models_with_metadata2/'\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_BASE = 3\n",
    "EPOCHS_SPECIALIST = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        row = self.dataframe.iloc[item]\n",
    "        \n",
    "        comment = str(row[TEXT_COLUMN])\n",
    "        app_version = str(row[APP_VERSION_COLUMN])\n",
    "        date_str = pd.to_datetime(row[AT_COLUMN]).strftime('%B %Y')\n",
    "        \n",
    "        combined_text = f\"version {app_version} date {date_str} comment: {comment}\"\n",
    "        label = row[LABEL_COLUMN]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            combined_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c642e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = ReviewDataset(\n",
    "        dataframe=df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce4f18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, device, n_epochs):\n",
    "    model = model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}')\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    return model\n",
    "\n",
    "def train_base_model(train_df, device):\n",
    "    print(\"\\n--- Training Base Model with Metadata ---\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    model = BertForSequenceClassification.from_pretrained(BASE_MODEL_NAME, num_labels=3)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    model = train_model(model, train_data_loader, optimizer, device, EPOCHS_BASE)\n",
    "    \n",
    "    print(f\"Base model training complete. Saving to {BASE_MODEL_PATH}\")\n",
    "    model.save_pretrained(BASE_MODEL_PATH)\n",
    "    tokenizer.save_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "def train_specialist_models(train_df, device):\n",
    "    print(\"\\n--- Training Specialist Models with Metadata ---\")\n",
    "    app_names = train_df['App'].unique()\n",
    "    \n",
    "    for app_name in app_names:\n",
    "        print(f\"\\nFine-tuning for app: {app_name}\")\n",
    "        \n",
    "        tokenizer = BertTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = BertForSequenceClassification.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        app_df = train_df[train_df['App'] == app_name]\n",
    "        \n",
    "        app_data_loader = create_data_loader(app_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        model = train_model(model, app_data_loader, optimizer, device, EPOCHS_SPECIALIST)\n",
    "        \n",
    "        specialist_path = os.path.join(SPECIALIST_MODELS_PATH, f'bert-specialist-{app_name}')\n",
    "        print(f\"Specialist model for {app_name} training complete. Saving to {specialist_path}\")\n",
    "        model.save_pretrained(specialist_path)\n",
    "        tokenizer.save_pretrained(specialist_path)\n",
    "\n",
    "def evaluate_models(test_df, device):\n",
    "    print(\"\\n--- Evaluating Specialist Models ---\")\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    app_names = test_df['App'].unique()\n",
    "\n",
    "    for app_name in app_names:\n",
    "        print(f\"Evaluating on test data for app: {app_name}\")\n",
    "        specialist_path = os.path.join(SPECIALIST_MODELS_PATH, f'bert-specialist-{app_name}')\n",
    "        \n",
    "        tokenizer = BertTokenizer.from_pretrained(specialist_path)\n",
    "        model = BertForSequenceClassification.from_pretrained(specialist_path)\n",
    "        model = model.to(device)\n",
    "        model = model.eval()\n",
    "\n",
    "        app_test_df = test_df[test_df['App'] == app_name]\n",
    "        \n",
    "        for _, row in app_test_df.iterrows():\n",
    "            true_label = row[LABEL_COLUMN]\n",
    "            \n",
    "            comment = str(row[TEXT_COLUMN])\n",
    "            app_version = str(row[APP_VERSION_COLUMN])\n",
    "            date_str = pd.to_datetime(row[AT_COLUMN]).strftime('%B %Y')\n",
    "            review_text = f\"version {app_version} date {date_str} comment: {comment}\"\n",
    "            \n",
    "            encoded_review = tokenizer.encode_plus(\n",
    "                review_text, max_length=MAX_LEN, add_special_tokens=True,\n",
    "                return_token_type_ids=False, padding='max_length', truncation=True,\n",
    "                return_attention_mask=True, return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded_review['input_ids'].to(device)\n",
    "            attention_mask = encoded_review['attention_mask'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                _, prediction = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            y_pred.append(prediction.item())\n",
    "            y_test.append(true_label)\n",
    "\n",
    "    print(\"\\n--- Final Evaluation Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Mixed', 'Positive']))\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"\\nF1 Macro Score: {f1_macro:.4f}\")\n",
    "def predict_on_new_data(predict_df, device):\n",
    "    print(\"\\n--- Predicting on New Data ---\")\n",
    "    predictions = []\n",
    "    app_names = predict_df['App'].unique()\n",
    "    \n",
    "    models = {}\n",
    "    tokenizers = {}\n",
    "    for app_name in app_names:\n",
    "        specialist_path = os.path.join(SPECIALIST_MODELS_PATH, f'bert-specialist-{app_name}')\n",
    "        if os.path.exists(specialist_path):\n",
    "            tokenizers[app_name] = BertTokenizer.from_pretrained(specialist_path)\n",
    "            models[app_name] = BertForSequenceClassification.from_pretrained(specialist_path)\n",
    "            models[app_name].to(device)\n",
    "            models[app_name].eval()\n",
    "\n",
    "    for _, row in predict_df.iterrows():\n",
    "        app_name = row['App']\n",
    "        \n",
    "        comment = str(row[TEXT_COLUMN])\n",
    "        app_version = str(row[APP_VERSION_COLUMN])\n",
    "        date_str = pd.to_datetime(row[AT_COLUMN]).strftime('%B %Y')\n",
    "        review_text = f\"version {app_version} date {date_str} comment: {comment}\"\n",
    "        \n",
    "        if app_name in models:\n",
    "            tokenizer = tokenizers[app_name]\n",
    "            model = models[app_name]\n",
    "            \n",
    "            encoded_review = tokenizer.encode_plus(\n",
    "                review_text, max_length=MAX_LEN, add_special_tokens=True,\n",
    "                return_token_type_ids=False, padding='max_length', truncation=True,\n",
    "                return_attention_mask=True, return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded_review['input_ids'].to(device)\n",
    "            attention_mask = encoded_review['attention_mask'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                _, prediction = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.append(prediction.item())\n",
    "        else:\n",
    "            predictions.append(None)\n",
    "\n",
    "    predict_df['predicted_sentiment'] = predictions\n",
    "    return predict_df\n",
    "\n",
    "def format_for_submission(results_df):\n",
    "    print(\"\\n--- Formatting for Submission ---\")\n",
    "    app_order = ['gpt', 'claude', 'deepseek', 'gemini', 'grok', 'perplexity']\n",
    "    results_df['App'] = pd.Categorical(results_df['App'], categories=app_order, ordered=True)\n",
    "    sorted_df = results_df.sort_values(by=['App', 'CommentId'])\n",
    "    \n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df['CommentId'] = sorted_df['App'].astype(str) + '_' + sorted_df['CommentId'].astype(str)\n",
    "    submission_df['Sentiment'] = sorted_df['predicted_sentiment']\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "490b77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 6 CSV files to process...\n",
      "  - Processed claude.csv and added 'claude' as App.\n",
      "  - Processed deepseek.csv and added 'deepseek' as App.\n",
      "  - Processed gemini.csv and added 'gemini' as App.\n",
      "  - Processed gpt.csv and added 'gpt' as App.\n",
      "  - Processed grok.csv and added 'grok' as App.\n",
      "  - Processed perplexity.csv and added 'perplexity' as App.\n",
      "\n",
      "Combining all DataFrames...\n",
      "Done.\n",
      "Found 6 CSV files to process...\n",
      "  - Processed claude.csv and added 'claude' as App.\n",
      "  - Processed deepseek.csv and added 'deepseek' as App.\n",
      "  - Processed gemini.csv and added 'gemini' as App.\n",
      "  - Processed gpt.csv and added 'gpt' as App.\n",
      "  - Processed grok.csv and added 'grok' as App.\n",
      "  - Processed perplexity.csv and added 'perplexity' as App.\n",
      "\n",
      "Combining all DataFrames...\n",
      "Done.\n",
      "\n",
      "--- Training Base Model with Metadata ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n",
      "Base model training complete. Saving to ./models/base_model_with_metadata2/\n",
      "\n",
      "--- Training Specialist Models with Metadata ---\n",
      "\n",
      "Fine-tuning for app: grok\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n",
      "Specialist model for grok training complete. Saving to ./models/specialist_models_with_metadata2/bert-specialist-grok\n",
      "\n",
      "Fine-tuning for app: gemini\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n",
      "Specialist model for gemini training complete. Saving to ./models/specialist_models_with_metadata2/bert-specialist-gemini\n",
      "\n",
      "Fine-tuning for app: gpt\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n",
      "Specialist model for gpt training complete. Saving to ./models/specialist_models_with_metadata2/bert-specialist-gpt\n",
      "\n",
      "Fine-tuning for app: perplexity\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n",
      "Specialist model for perplexity training complete. Saving to ./models/specialist_models_with_metadata2/bert-specialist-perplexity\n",
      "\n",
      "Fine-tuning for app: deepseek\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n",
      "Specialist model for deepseek training complete. Saving to ./models/specialist_models_with_metadata2/bert-specialist-deepseek\n",
      "\n",
      "Fine-tuning for app: claude\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n",
      "Specialist model for claude training complete. Saving to ./models/specialist_models_with_metadata2/bert-specialist-claude\n",
      "\n",
      "--- Evaluating Specialist Models ---\n",
      "Evaluating on test data for app: gemini\n",
      "Evaluating on test data for app: gpt\n",
      "Evaluating on test data for app: grok\n",
      "Evaluating on test data for app: deepseek\n",
      "Evaluating on test data for app: perplexity\n",
      "Evaluating on test data for app: claude\n",
      "\n",
      "--- Final Evaluation Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.74      0.77      3504\n",
      "       Mixed       0.28      0.15      0.19      1118\n",
      "    Positive       0.94      0.97      0.95     21555\n",
      "\n",
      "    accuracy                           0.90     26177\n",
      "   macro avg       0.67      0.62      0.64     26177\n",
      "weighted avg       0.89      0.90      0.90     26177\n",
      "\n",
      "\n",
      "F1 Macro Score: 0.6382\n",
      "\n",
      "--- Predicting on New Data ---\n",
      "\n",
      "--- Formatting for Submission ---\n",
      "\n",
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    train_set = import_data('Train/')\n",
    "    test_set = import_data('Test/') \n",
    "\n",
    "    train, val = train_test_split(train_set, test_size=0.2, random_state=42, stratify=train_set['Sentiment'])\n",
    "    \n",
    "    if not os.path.exists(BASE_MODEL_PATH):\n",
    "        os.makedirs(BASE_MODEL_PATH)\n",
    "    if not os.path.exists(SPECIALIST_MODELS_PATH):\n",
    "        os.makedirs(SPECIALIST_MODELS_PATH)\n",
    "\n",
    "    train_base_model(train, device)\n",
    "    train_specialist_models(train, device)\n",
    "    evaluate_models(val, device)\n",
    "    \n",
    "    prediction_results_df = predict_on_new_data(test_set, device)\n",
    "    submission = format_for_submission(prediction_results_df)\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"\\nSubmission file 'submission.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
